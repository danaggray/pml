---
title: "PML_ExcercisePrediction"
author: "dgg"
date: "December 25, 2015"
output: html_document
---
##Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Assignment

We are to create a model to predict the manner in which the subjects performed the exercise.This is the "classe" variable in the training set. We will then use the model to predict 20 different test cases.

## Load and Prepare Training Data
Load required libraries
```{r}

library(caret)
library(rpart)
library(rpart.plot)
```


```{r}
trainData <- read.csv("C:\\Projects\\Coursera\\Practical Machine Learning\\Project\\pml-training.csv", na.strings = c("NA", ""))
```

## Load Test Data

```{r, echo=FALSE}
testData <- read.csv("C:\\Projects\\Coursera\\Practical Machine Learning\\Project\\pml-testing.csv", na.strings = c("NA", ""))

```
Preliminary investigation and evaluation of the data:
```{r}
# str(trainData)
# summary(trainData)


```
We can remove several variables that provide no useful data, such as the test subject ('user_name'), formatted date ('cvtd_timestamp' - this data is a factor that will be generated from the raw time stamp variables), and row number ('X')

```{r}
newIndex <- grep("X|user_name|cvtd_timestamp", names(trainData))
trainData <- trainData[, -newIndex]

```


We will remove variables that have virtually no variance as they would not provide significant meaningful information for making an accurate prediction model.
```{r}
nzData <- nearZeroVar(trainData)
trainData <- trainData[, -nzData]

```
There are a number of variables that are relatively incomplete in that they have a large number of 'NA's. Removing these will help in making a more accurate predicting model.

```{r}
naData <- apply(trainData, 2, function(x) {
    sum(is.na(x))
})
trainData <- trainData[, which(naData == 0)]

```
## Cross Validation
For the purposes of cross validation We subset the resulting data into a training set and test set (p=.75)

```{r}
subIndex <- createDataPartition(y = trainData$classe, p = 0.75, list = FALSE)
trainSubData <- trainData[subIndex, ] 
testSubData <- trainData[-subIndex, ]  # cross validation test set
```

Try a Decision Tree predicting model
```{r}
dcModel <- rpart(classe ~ ., data=trainSubData, method="class")

```
Generate Prediction
```{r}
dcPrediction <- predict(dcModel, testSubData, type = "class")

```
Plot Prediction
```{r}
rpart.plot(dcModel, main="Classification Tree", extra=102, under=TRUE, faclen=0)

```

Run model against our subsetted test data
```{r}
confusionMatrix(dcPrediction, testSubData$classe)

```
This model generates pretty good results.
82% accuracy etc.

Next we try the Random Forest approach

```{r}
library(randomForest)

rfmodel <- randomForest(classe ~. , data=trainSubData, method="class")

rfPrediction <- predict(rfmodel, testSubData, type = "class")

confusionMatrix(rfPrediction, testSubData$classe)

```
We can see that the Random Forest model works better than the Decision Tree model as a predictor, .998 vs .820. The expected OOB (1-accuracy) error is .2%. Confidence Interval is 0.9968 - 0.9993
The RF model is well suited for prediction against the test set which is a select group of 6 relatively healthy young individuals. We these conditions we can expect an accuracy of 99.8%. Naturally against a more representative set of the population the model would probably not perform at that level.

Now let us predict against our testing set
```{r}
# run data cleaning processes against the original test set

newIndex <- grep("X|user_name|cvtd_timestamp", names(testData))
testData <- testData[, -newIndex]
nzData <- nearZeroVar(testData)
testData <- testData[, -nzData]

naData <- apply(testData, 2, function(x) {
    sum(is.na(x))
})
testData <- testData[, which(naData == 0)]

```

Run the prediction against the test data

```{r}
prediction <- predict(rfmodel, testData, type="class")
prediction
```

Now write out the files for project credit

```{r}
# Write files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(prediction)

```
Using our model to predict the 20 test cases we had 100% success. 
